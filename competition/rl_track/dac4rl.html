<h1> DAC4RL Scenario Details </h1>
<p>In this challenge, we cast the problem of adaptively setting hyperparameters in Reinforcement Learning (RL) as a Dynamic Algorithm Configuration (DAC) problem.</p>
<p>In our DAC for RL scenario, we are given a <b> target agent </b> or <b>inner agent</b> with one or more <b> dynamically reconfigurable hyperparameters </b> and our objective is to find a <b> DAC policy </b> or <b>outer agent</b> that chooses a valid hyperparameter setting at multiple <b> reconfiguration points </b> during training of the inner agent so as to maximize a given <b>reward metric</b> across a given <b> target environment distribution</b>. Each environment drawn from this distribution is termed a problem <b>instance</b> or <b>inner environment</b>. Each such environment is created by varying various <b>context features</b> of a given environment. The problem instance in our case is a Gym Environment, and more specifically, a CARL environment which has a set of context features that can be varied. The target environment distribution consists of the following 5 CARL environments, each with their own set of context features: <i>CARLPendulumEnv, CARLAcrobotEnv, CARLMountainCarContinuousEnv, CARLLunarLanderEnv, CARLCartPoleEnv.</i></p>

<p>The DAC policy acts in an <b>outer environment</b> which is also implemented as a Gym Environment and randomly samples one of the target environments with a different set of contexts at reset. The target agent is then required to act on a sampled problem instance. Each step in the outer environment, also called an epoch, corresponds to 1/10 of the total timesteps for training in the target environment in the our setup. Thus, the DAC policy needs to set the target agent's hyperparameters at each such outer step, i.e. reconfiguration point, after which the agent is trained for one epoch and then evaluated separately on an evaluation environment for 100 rollouts. The (outer) environment then returns the mean reward of these 100 rollouts as the (outer) reward, along with the <i>done</i> flag for whether the timesteps have been exhausted or not. Additionally, a state is returned to the outer agent, which is a dictionary that consists of a counter for the current epoch, the standard deviation of the evaluation rewards, a list of the rewards obtained during training and the mean lengths of the training episodes. Further, the reset function returns the name of the instantiated environment, the names of the changing context features, the values of all the defined context features and the standard deviation of the distribution from which these contexts are sampled. The DAC policy needs to set the hyperparameters such that the (outer) reward is maximized for each problem instance it encounters. The metric we employ <b>per instance</b> is the collected (outer) reward which corresponds to maximizing the anytime performance of the evaluation rollouts of the inner agent. The metric we employ for the performance of a submitted DAC policy <b>across the instances</b> is the average of the ranks obtained by the policy on instances of each target environment. (The ranking is calculated among all the submissions received so far, with only one submission being considered for each team.)</p>


<p>We now detail each of the DAC components for the DAC scenario we consider in the DAC4RL challenge:</p>
<h3> Target Agent </h3>
For this competition, the target agent implements a Stable Baselines 3 training pipeline optimizing the parameters of a deep RL agent. We allow the target to be selected from among 3 common algorithms implemented in Stable Baselines 3: <i>DDPG, PPO</i> and <i>SAC</i>.
<h3> Dynamically Reconfigurable Parameters </h3>
These constitute the action space of the outer environment and are passed as a dict (See https://github.com/automl-private/DAC4RL/blob/main/rlenv/RLEnv.py). The dict consists of the name of the algorithm (one of the 3 mentioned above) to be used for the next epoch along with all the respective hyperparameters from the Stable Baselines 3 implementations. However, it needs to be noted that certain hyperparameters can only be applied at the first time an agent is instantiated, and thus, cannot be changed afterwards(For example, the clip range. for more details, please see the documentation for the hyperparameters in sb3 at https://stable-baselines3.readthedocs.io/en/master/).
<h3> Reconfiguration Points </h3>
At every outer environment step, the action consists of the selected inner agent's algorithm and the corresponding hyperparameters being set. The outer environment keeps a track of the agents instantiated so far and warm-starts the agent to be applied at the next outer environment step with the last instantiated agent for the selected algorithm type (i.e., the Neural Network parameters are set to the same values as the last step when the selected algorithm was used but the hyperparameters are set to the newly passed values). In the meta-environment, this is tracked using a distionary of already instantiated models which is queried each time an algorithm is set.
<h3> DAC Policy </h3>
This is the policy that needs to be designed and submitted by the participants of the competition. It acts by setting the hyperparameters of the target agent. The policy can be, e.g., any manually designed heuristic such as an annealed learning rate schedule or it could even be something automatic that sets an online hyperparameter schedule by learning across different problem instances.
<!-- The objective of this competition to create a dynamic schedule of selected agents and corresponding hyperparameters. You are free to stick with one agent and only learning rate schedule, or change multiple hyperparameters, or even go for a schedule of selected agents and corresponding hyperparameters. -->
<h3> Target Environment Distribution </h3>
<ul>
<li>CARLPendulumEnv</li>
<li>CARLAcrobotEnv</li>
<li>CARLMountainCarContinuousEnv</li>
<li>CARLLunarLanderEnv</li>
<li>CARLCartPoleEnv</li>
</ul>

<p>The context features that vary for each of these environments can be found in the generato≈ï file in the competition repository (see https://github.com/automl-private/DAC4RL/blob/main/rlenv/generators.py).


<p>
We provide some examples of possible solution policies in the competition repo (See https://github.com/automl-private/DAC4RL/tree/main/baselines). The challenge is looking for creative solutions and therefore aims to minimally constrain the policy space. That being said, your solution policy should...
<ul>
<li>correctly implement the standard DACPolicy interface</li>
<li>not transfer experience across test runs </li> (TODO: clarify)
<li>only interact with the training pipeline by changing the hyperparameters (including the algorithm). In particular, while it may read, it may not modify the instance or dynamic state (TODO: clarify what is dynamic state) of the training pipeline in any way.</li>
<li>impose limited overhead as to not violate the evaluation resource constraints(TODO: link).</li>
<li>not explicitly overfit the target instance distribution (TODO: clarify), i.e. it should not fail when applied to an out of distribution instance. For example, with context values sampled from a different distribution than the one provided during training.</li>
<li>not rely on information that would not be available in a real world setting, e.g., info about the test set.</li>
</ul>

The organizers retain the right to disqualify submissions whose solution policy violates these constraints. Note that some constraints may leave room for interpretation, in doubt, please contact one of the organizers.
</p>
