<h1> DAC4RL Scenario Details </h1>
<p>In this challenge, we cast the problem of adaptively setting hyperparameters in Reinforcement Learning (RL) as a Dynamic Algorithm Configuration (DAC) problem.</p>
<p>In a DAC for RL scenario, we are given a <b> target agent </b> or <b>inner agent</b> with one or more <b> dynamically reconfigurable hyperparameters </b> and our objective is to find a <b> DAC policy </b> or <b>outer agent</b> that chooses a valid hyperparameter setting at every <b> reconfiguration point </b> during training of the inner agent so as to maximize a given <b> reward metric </b> across a given <b> target environment distribution</b>. Each environment drawn from this distribution is termed a problem <b>instance</b> or an <b>inner environment</b>. Each such environment is created by varying various <b>context features</b> of a given environment. The problem instance in our case is a Gym Environment, and more specifically, a CARL environment which has a set of context features that can be varied. The target environment distribution consists of the following 5 CARL environments, each with their own set of context features: CARLPendulumEnv	CARLAcrobotEnv	CARLMountainCarContinuousEnv	CARLLunarLanderEnv	CARLCartPoleEnv. </p>

<p>The DAC policy acts in an <b>outer environment</b> which is also implemented as a Gym Environment and randomly samples one of the target environments with a different set of changing contexts at reset. The target agent is then required to act on a sampled problem instance. Each step in the outer environment, also called an epoch, corresponds to 1/10 of the total timesteps for training in the target environment. Thus, the DAC policy needs to set the target agent's hyperparameters at each such outer step, after which the agent is trained for one epoch and then evaluated seperately on an evaluation environment for 100 rollouts. The (outer) environment then returns the mean reward of these 100 rollouts, along with the flag for whether the timesteps have been exhausted or not. Additionally, a state is returned to the outer agent, which is a dictionary that consists of a counter for the current epoch, the standard deviation of the evaluation rewards, a list of hte rewards obtained during training and the mean lenghts of the training episodes. Moreover, the reset function returns the name of the instantiated environment, the names of the changing context features, the values of all the defined context features nad the standard deviation of the distribution from which these contexts are sampled. The DAC policy needs to set the hyperparameters such that the (outer) reward is maximized for each problem instance it encounters. The metric we employ <b>per instance</b> is the Area Under the Curve (AUC) of the learning curve of the collected (outer) reward. The metric we employ for the performance of a submitted DAC policy <b>across the instances</b> is the average of the ranks obtained by the policy on instances of each target environment. (The ranking is calculated among all the submissions received so far, with only one submission being considered for each team.)</p>


<p>We now detail each of the DAC components for the DAC scenario we consider in the DAC4RL challenge:</p>
<h3> Target Agent </h3>
The target agent implements a Stable Baselines 3 training pipeline optimizing the parameters of a deep RL agent to maximize the collected reward in an environment selected from the target environment distribution. For this competition, we allow the target to be selected from among 3 common algorithms: DDPG, PPO and SAC. 
<h3> Dynamically Reconfigurable Parameters </h3>
For each of th e agents listed above, all the respective hyperparameters are dynamically reconfigurable. However, it needs to be noted that certain hyperparameters can only be applied at the first time an agent is instantiated, and thus, cannot be changed afterwards. 
<h3> Reconfiguration Points </h3>
At every epoch, the action consists of the selected inner agent and the corresponding hyperparameters being set. The outer environment keeps a track of the agents instantiated so far and restarts from the last instantiated agent. 
<h3> Target Problem Distribution </h3>
<p> The objective of this competition to create a dynamic schedule of selected agents and corresponding hyperparameters. You are free to stick with one agent and only learning rate schedule, or change multiple hyperparameters, or even go for a schedule of selected agents and corresponding hyperparameters.  

We provide some examples of possible solution policies in the competition repo. Note that the challenge is looking for creative solutions and therefore aims to minimally constrain the policy space. That being said, your solution policy should ...
<ul>
<li>correctly implement the standard DACPolicy interface</li>
<li>Not transfer experience across test runs </li>
<li>only interact with the training pipeline by changing the hyperparameters. In particular, while it may read, it may not modify the instance or dynamic state of the training pipeline in any way.</li>
<li>impose limited overhead as to not violate the evaluation resource constraints.</li>
<li>not explicitely overfit the target instance distribution, i.e. it should not fail when applied to an out of distribution instance. For example, with context values sampled from a different distribution than the one provided during training.</li>
<li>not rely on information that would be not available in a real world setting, e.g., info about the test set.</li>
</ul>

The organizers retain the right to disqualify submissions whose solution policy violates these constraints. Note that some constraints may leave room for interpretation, in doubt, please contact one of the organizers.
</p>
